{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLnet for Rosenbrock\n",
    "\n",
    "This notebook contains a tutorial on training Lipschitz-bounded deep networks with PLnet from [Wang & Dvijotham & Manchester (2024)](https://arxiv.org/html/2402.01344v2). We'll demonstrate how to train PL networks on the [Rosenbrock](https://en.wikipedia.org/wiki/Rosenbrock_function) function.\n",
    "\n",
    "This notebook serves as an initial idea/simple example of training PLnet and implement the inverse. Better example might be provided later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dependency\n",
    "The plnet in torch with version is mostly based on numpy and pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Union, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scipy.io\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from flax import linen \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "code to generate Rosenbrock data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rosenbrock(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Rosenbrock function for torch tensors.\n",
    "    \"\"\"\n",
    "    f = lambda x, y: (x - 1.) ** 2 / 200. + 0.5 * (y - x ** 2) ** 2\n",
    "\n",
    "    single = x.ndim == 1\n",
    "    if single:\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "    N = x.shape[-1]\n",
    "    y = torch.stack([f(x[..., i], x[..., i+1]) for i in range(N - 1)], dim=1)\n",
    "    y = torch.mean(y, dim=1)\n",
    "\n",
    "    if single:\n",
    "        y = y.squeeze(0)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def Sampler(\n",
    "        batches: int,\n",
    "        data_dim: int,\n",
    "        x_min: Union[float, torch.Tensor] = -2.,\n",
    "        x_max: Union[float, torch.Tensor] = 2.,\n",
    "        device: str = \"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Uniform sampler similar to JAX random.uniform.\n",
    "    \"\"\"\n",
    "    return (x_max - x_min) * torch.rand((batches, data_dim), device=device) + x_min\n",
    "\n",
    "\n",
    "def data_gen(\n",
    "    data_dim: int = 20,\n",
    "    val_min: float = -2.,\n",
    "    val_max: float = 2.,\n",
    "    train_batch_size: int = 200,\n",
    "    test_batch_size: int = 5000,\n",
    "    train_batches: int = 200,\n",
    "    test_batches: int = 1,\n",
    "    eval_batch_size: int = 5000,\n",
    "    eval_batches: int = 100,\n",
    "    device: str = \"cpu\"\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate synthetic train/test/eval datasets for the Rosenbrock function.\n",
    "    \"\"\"\n",
    "    # Sample points\n",
    "    xtrain = Sampler(train_batch_size * train_batches, data_dim, x_min=val_min, x_max=val_max, device=device)\n",
    "    xtest  = Sampler(test_batch_size * test_batches, data_dim, x_min=val_min, x_max=val_max, device=device)\n",
    "    xeval  = Sampler(eval_batch_size * eval_batches, data_dim, x_min=val_min, x_max=val_max, device=device)\n",
    "\n",
    "    # Compute labels\n",
    "    ytrain, ytest, yeval = Rosenbrock(xtrain), Rosenbrock(xtest), Rosenbrock(xeval)\n",
    "\n",
    "    data = {\n",
    "        \"xtrain\": xtrain,\n",
    "        \"ytrain\": ytrain,\n",
    "        \"xtest\": xtest,\n",
    "        \"ytest\": ytest,\n",
    "        \"xeval\": xeval,\n",
    "        \"yeval\": yeval,\n",
    "        \"train_batches\": train_batches,\n",
    "        \"train_batch_size\": train_batch_size,\n",
    "        \"test_batches\": test_batches,\n",
    "        \"test_batch_size\": test_batch_size,\n",
    "        \"eval_batches\": eval_batches,\n",
    "        \"eval_batch_size\": eval_batch_size,\n",
    "        \"data_dim\": data_dim\n",
    "    }\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model & Data Configuration\n",
    "\n",
    "Configure the data dimension and learning parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = 20\n",
    "lr_max = 1e-2\n",
    "epochs = 100\n",
    "n_batch = 50\n",
    "depth = 2 \n",
    "layer_size = [256]*8\n",
    "tau=2\n",
    "\n",
    "\n",
    "name = 'BiLipNet_torch'\n",
    "root_dir =  f'{os.getcwd()}/plnet/results_exp/{name}-rosenbrock-dim{data_dim}-batch{n_batch}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xtrain': tensor([[-1.2666, -1.0204,  0.7798,  ...,  0.6958,  0.9287,  1.5379],\n",
      "        [-1.0117, -0.8180,  1.7388,  ..., -0.1832,  0.7874,  1.9441],\n",
      "        [-0.8629,  1.5679, -0.6890,  ..., -0.7470,  0.1682, -1.8098],\n",
      "        ...,\n",
      "        [-1.3806, -1.3370,  0.8478,  ...,  0.7322, -1.3290,  0.5042],\n",
      "        [ 0.2738, -0.8625, -1.2013,  ...,  0.5024, -0.0582, -0.0217],\n",
      "        [-0.5152, -0.0226,  0.2163,  ..., -0.2501,  1.0713, -1.4838]]), 'ytrain': tensor([1.2955, 1.0681, 1.5642,  ..., 4.1177, 2.3219, 0.9298]), 'xtest': tensor([[ 1.8743, -0.4425,  0.8785,  ..., -1.6351, -0.7958,  1.5558],\n",
      "        [-1.0153, -0.0483, -0.7021,  ..., -0.6903,  1.0848, -0.8744],\n",
      "        [ 1.3316, -1.2648, -1.2832,  ...,  0.2153,  1.0958,  1.3890],\n",
      "        ...,\n",
      "        [ 1.9334, -0.7758,  1.1866,  ..., -1.4052,  0.6648,  0.8666],\n",
      "        [ 0.9033,  1.8937,  1.6830,  ..., -1.1012, -1.5068, -0.5422],\n",
      "        [ 0.9079, -0.7346,  1.8269,  ..., -1.2821, -1.1292, -1.1362]]), 'ytest': tensor([1.9030, 1.1314, 2.7980,  ..., 1.7582, 1.7993, 3.2038]), 'xeval': tensor([[ 1.2041, -1.4924,  0.4536,  ...,  0.6993,  1.6363, -1.0316],\n",
      "        [ 1.5489,  1.9984,  0.7462,  ..., -1.2175, -1.1409, -0.5012],\n",
      "        [-0.7801,  1.9273, -1.7902,  ...,  0.3786,  1.0301,  0.3703],\n",
      "        ...,\n",
      "        [-1.3687, -1.3655,  0.5798,  ..., -1.9657,  1.4811, -0.1754],\n",
      "        [-0.2949, -1.9497,  0.2665,  ...,  1.0589, -1.8580, -1.1454],\n",
      "        [-1.7286, -1.8088, -0.7811,  ...,  1.3286,  0.8182, -1.5851]]), 'yeval': tensor([2.1291, 2.0850, 4.0217,  ..., 2.1637, 3.4628, 2.4301]), 'train_batches': 50, 'train_batch_size': 200, 'test_batches': 1, 'test_batch_size': 5000, 'eval_batches': 5, 'eval_batch_size': 500, 'data_dim': 20}\n",
      "torch.Size([10000, 20])\n"
     ]
    }
   ],
   "source": [
    "data= data_gen(data_dim, train_batches=n_batch, eval_batch_size=500,eval_batches=5)\n",
    "\n",
    "# visualize the data\n",
    "print(data)\n",
    "print(data['xtrain'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "train model for rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train function\n",
    "define train function with configurable loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_flexible_loss(\n",
    "    model: nn.Module,\n",
    "    data: dict,\n",
    "    fitness_func,  # function(model, x, y) -> loss\n",
    "    fitness_eval_func,  # function(model, x, y) -> eval_loss\n",
    "    name: str = 'bilipnet',\n",
    "    train_dir: str = './results/rosenbrock-nd',\n",
    "    lr_max: float = 1e-3,\n",
    "    epochs: int = 600,\n",
    "    figure_generation_function=lambda model, epoch: (model, epoch),\n",
    "    figure_generation_period=50,\n",
    "    device: str = 'cpu',\n",
    "):\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    ckpt_dir = os.path.join(train_dir, 'ckpt')\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    train_batches = data['train_batches']\n",
    "    train_batch_size = data['train_batch_size']\n",
    "\n",
    "    train_size = train_batches * train_batch_size\n",
    "    idx_shp = (train_batches, train_batch_size)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Count parameters\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f'model: {name}, size: {param_count/1e6:.2f}M')\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_max)\n",
    "    # Optionally: implement a linear schedule manually if needed\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=lr_max, total_steps=train_batches*epochs, pct_start=0.25\n",
    "    )\n",
    "\n",
    "    train_loss, val_loss = [], []\n",
    "    Lipmin, Lipmax, Tau = [], [], []\n",
    "\n",
    "    xtrain = data['xtrain'].to(device)\n",
    "    ytrain = data['ytrain'].to(device)\n",
    "    xtest = data['xtest'].to(device)\n",
    "    ytest = data['ytest'].to(device)\n",
    "    xeval = data['xeval'].to(device)\n",
    "    yeval = data['yeval'].to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle indices\n",
    "        idx = torch.randperm(train_size, device=device).view(idx_shp)\n",
    "        tloss = 0.\n",
    "\n",
    "        for b in range(train_batches):\n",
    "            x = xtrain[idx[b, :], :]\n",
    "            y = ytrain[idx[b, :]]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = fitness_func(model, x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # update LR if using scheduler\n",
    "\n",
    "            tloss += loss.item()\n",
    "\n",
    "        tloss /= train_batches\n",
    "        train_loss.append(tloss)\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vloss = fitness_eval_func(model, xtest, ytest)\n",
    "        val_loss.append(vloss.item())\n",
    "        model.train()\n",
    "\n",
    "        # Example: get bounds if model has such method\n",
    "        if hasattr(model, 'get_bounds'):\n",
    "            lipmin, lipmax, tau = model.get_bounds()\n",
    "            Lipmin.append(lipmin)\n",
    "            Lipmax.append(lipmax)\n",
    "            Tau.append(tau)\n",
    "        else:\n",
    "            Lipmin.append(0.0)\n",
    "            Lipmax.append(0.0)\n",
    "            Tau.append(0.0)\n",
    "\n",
    "        if epoch % figure_generation_period == 0:\n",
    "            figure_generation_function(model, epoch)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:3d} | loss: {tloss:.3f}/{vloss:.3f}, tau: {Tau[-1]:.3f}, Lip: {Lipmin[-1]:.3f}/{Lipmax[-1]:.2f}')\n",
    "\n",
    "    # Eval loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        eloss = fitness_eval_func(model, xeval, yeval)\n",
    "    print(f'{name}: eval loss: {eloss:.3f}')\n",
    "\n",
    "    # Save metrics to data dict\n",
    "    data['train_loss'] = torch.tensor(train_loss)\n",
    "    data['val_loss'] = torch.tensor(val_loss)\n",
    "    data['lipmin'] = torch.tensor(Lipmin)\n",
    "    data['lipmax'] = torch.tensor(Lipmax)\n",
    "    data['tau'] = torch.tensor(Tau)\n",
    "    data['eval_loss'] = torch.tensor(eloss)\n",
    "\n",
    "    scipy.io.savemat(os.path.join(train_dir, 'data.mat'), data)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(ckpt_dir, 'params.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Configuration\n",
    "Define loss function. Here we keep if as simple. You can choose MSE for regresion or entropy loss for classification. Here we use MSE for Rosbenbrock function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitness_loss(\n",
    "    optimal_point: torch.Tensor = None,\n",
    "    threshold_value: float = 0.5,\n",
    "    is_optimal: bool = False,\n",
    "    is_regression: bool = True,   # choose loss type\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a loss function compatible with PyTorch models.\n",
    "    \"\"\"\n",
    "\n",
    "    def fitloss(model, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute loss for a batch of inputs x and targets y.\n",
    "        \"\"\"\n",
    "        # Apply the model\n",
    "        if is_optimal and optimal_point is not None:\n",
    "            yh = model(x, optimal_point)  # model should handle optional extra argument\n",
    "        else:\n",
    "            yh = model(x)\n",
    "\n",
    "        # Regression or classification\n",
    "        if is_regression:\n",
    "            loss = F.mse_loss(yh, y, reduction='mean')\n",
    "        else:\n",
    "            # Binary classification with threshold\n",
    "            # Compute sigmoid BCE loss\n",
    "            loss = F.binary_cross_entropy_with_logits(yh - threshold_value, y.float(), reduction='mean')\n",
    "\n",
    "        return loss\n",
    "\n",
    "    return fitloss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robustnn.bilipnet_torch import BiLipNet\n",
    "from robustnn.plnet_torch import PLNet\n",
    "\n",
    "train_dir = f'{root_dir}/{name}-{depth}-tau{tau}'\n",
    "mu = 0.1\n",
    "nu = 10\n",
    "block = BiLipNet(data_dim, layer_size, mu=0.1, nu=10, depth=depth, tau=tau, is_tau_fixed=True)\n",
    "model = PLNet(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: BiLipNet_torch, size: 2.05M\n",
      "Epoch:   1 | loss: 0.203/0.159, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:   2 | loss: 0.101/0.100, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:   3 | loss: 0.064/0.083, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:   4 | loss: 0.058/0.088, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:   5 | loss: 0.078/0.114, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:   6 | loss: 0.106/0.159, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:   7 | loss: 0.154/0.165, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:   8 | loss: 0.166/0.195, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:   9 | loss: 0.192/0.165, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  10 | loss: 0.152/0.205, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  11 | loss: 0.144/0.138, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  12 | loss: 0.129/0.134, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  13 | loss: 0.127/0.136, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  14 | loss: 0.137/0.131, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  15 | loss: 0.123/0.131, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  16 | loss: 0.107/0.104, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  17 | loss: 0.102/0.123, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  18 | loss: 0.105/0.105, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  19 | loss: 0.092/0.085, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  20 | loss: 0.084/0.088, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  21 | loss: 0.064/0.062, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  22 | loss: 0.054/0.062, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  23 | loss: 0.048/0.055, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  24 | loss: 0.053/0.070, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  25 | loss: 0.048/0.050, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  26 | loss: 0.039/0.045, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  27 | loss: 0.032/0.040, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  28 | loss: 0.029/0.043, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  29 | loss: 0.029/0.037, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  30 | loss: 0.026/0.034, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  31 | loss: 0.024/0.037, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  32 | loss: 0.024/0.033, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  33 | loss: 0.024/0.032, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  34 | loss: 0.019/0.029, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  35 | loss: 0.020/0.029, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  36 | loss: 0.018/0.027, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  37 | loss: 0.017/0.026, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  38 | loss: 0.015/0.025, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  39 | loss: 0.014/0.023, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  40 | loss: 0.014/0.023, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  41 | loss: 0.013/0.025, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  42 | loss: 0.012/0.022, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  43 | loss: 0.012/0.024, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  44 | loss: 0.012/0.021, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  45 | loss: 0.011/0.024, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  46 | loss: 0.010/0.021, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  47 | loss: 0.010/0.019, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  48 | loss: 0.009/0.019, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  49 | loss: 0.008/0.020, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  50 | loss: 0.008/0.018, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  51 | loss: 0.007/0.017, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  52 | loss: 0.005/0.015, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  53 | loss: 0.004/0.015, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  54 | loss: 0.003/0.014, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  55 | loss: 0.003/0.014, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  56 | loss: 0.002/0.014, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  57 | loss: 0.002/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  58 | loss: 0.002/0.014, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  59 | loss: 0.001/0.014, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  60 | loss: 0.001/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  61 | loss: 0.001/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  62 | loss: 0.001/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  63 | loss: 0.001/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  64 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  65 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  66 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  67 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  68 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  69 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  70 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  71 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  72 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  73 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  74 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  75 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  76 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  77 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  78 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  79 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  80 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  81 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  82 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  83 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  84 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  85 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  86 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  87 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  88 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  89 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  90 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  91 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  92 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  93 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  94 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  95 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  96 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  97 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  98 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch:  99 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "Epoch: 100 | loss: 0.000/0.013, tau: 0.000, Lip: 0.000/0.00\n",
      "BiLipNet_torch: eval loss: 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10745/3329088172.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data['eval_loss'] = torch.tensor(eloss)\n"
     ]
    }
   ],
   "source": [
    "# Define loss function\n",
    "fitness_func_pl = get_fitness_loss(model)\n",
    "\n",
    "train_with_flexible_loss( model, data, fitness_func_pl, fitness_eval_func=fitness_func_pl, name=name, \n",
    "                         train_dir=train_dir, lr_max=lr_max, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters restored successfully.\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(train_dir, 'ckpt')\n",
    "ckpt_path = os.path.join(ckpt_dir, 'params.pt')\n",
    "\n",
    "# Load the saved parameters\n",
    "if os.path.exists(ckpt_path):\n",
    "\tmodel.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "\tmodel.eval()  # set model to evaluation mode\n",
    "\tmodel.to(device)\n",
    "\tprint(\"Model parameters restored successfully.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation - Inverse\n",
    "Inverse the model and compare. We sample 10 points with 20 dimension as z. Run the inverse on it inv(z). Then, we campare the forward of the inverse with the original value z. Ideally, z = PLNet ( inv (z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define inverse parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 800\n",
    "alpha = 0.1\n",
    "Lambda = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample data z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Sampler( 10, 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse z: [[-0.7266202   0.41303885  0.1398462  -0.0871107   0.84646493  1.520392\n",
      "   1.1599947   1.3756735   0.8531401   0.60397243  0.4329725   0.33819008\n",
      "   1.4826813   0.52883506  1.0807513   1.6351691   2.1282651   2.0434139\n",
      "   3.6237276   6.4443817 ]\n",
      " [-1.4208779   1.8727071   1.3566608   1.0139096   1.345627    0.4884618\n",
      "   1.2530352   0.48576882  1.7153794   0.8822845   0.7155441   0.872139\n",
      "   0.7797189   2.0450435   1.1782582   0.50292677  1.0747726   1.0052083\n",
      "  -0.26609278  5.148472  ]\n",
      " [ 1.209498    1.8629102   1.485064    0.3392176   0.84985745  0.77795625\n",
      "   0.08917625 -0.06314468  0.6732873  -1.411096    1.6455653   1.0673857\n",
      "  -1.0541252   1.3409362   1.2574241   1.8078521   1.4726223   2.2568402\n",
      "   3.0292487   5.7099743 ]\n",
      " [-1.0098727   0.7919287   0.5293421  -1.3831154   0.5609558   1.2761406\n",
      "   2.048948    1.5843995   1.5466778   0.9357493   0.43859237 -0.35181242\n",
      "  -0.24067342 -1.3541054   1.2931135   1.346929    1.0012894  -0.05674374\n",
      "   0.9728743   6.466585  ]\n",
      " [-0.54478455  0.3531075   0.5074605   1.4931538   1.6149564   1.1316001\n",
      "  -0.61737835  2.051634    1.0251667  -1.254845    1.5343006   0.9101643\n",
      "   0.69613016  0.55312395  0.97014034  1.0787426   0.60082656  1.4204409\n",
      "   2.3395934   4.6673937 ]\n",
      " [ 0.7524577   0.5978564  -1.1656313   1.0106306   1.9529052   1.4605479\n",
      "   1.6191937   0.96968615  0.35864758 -1.1085011   0.9748774  -0.65877604\n",
      "   0.7047386  -0.6070894   0.96744746  1.3912208   1.7495703   1.499906\n",
      "   2.4132974   6.012895  ]\n",
      " [-0.06740451  0.5968708   0.68152976  0.38482627  2.1667      1.4540919\n",
      "   1.7409936   0.72484475 -0.79625523  0.09714983  0.3992492   1.1034603\n",
      "   0.42362595  1.3959262   1.0264485  -0.77636206  2.2192836   1.7619301\n",
      "   2.6125033   6.003196  ]\n",
      " [ 1.002102    0.3115      1.2619916   0.77587557 -0.14048994 -1.251931\n",
      "   0.5202297  -0.4650018  -0.8673867   0.5483807   0.21447414 -0.05060136\n",
      "  -0.46599197  0.5098601   0.29024178  1.6885358   1.8464983   1.6030233\n",
      "   2.655222    5.974162  ]\n",
      " [ 0.09508389 -0.9826347   0.16572937 -0.77302516 -0.69060767 -0.1129393\n",
      "  -0.06220312  0.41460085  0.56764805  0.835178    1.0396314   0.5703543\n",
      "   1.9188204   1.1319252   0.873726    1.8839035   1.639858    1.0730952\n",
      "   0.71415794  6.1035843 ]\n",
      " [ 1.6001599   1.472783    1.5480675   1.3658892   0.10375917  0.6712219\n",
      "   0.8301687   0.8109732  -0.7428974  -0.22639354 -1.2806679   1.0506438\n",
      "   0.3422665   0.53700006  0.18184595  0.75900227 -1.0514503   2.0114708\n",
      "   3.1160064   4.5960083 ]] with shape (10, 20)\n"
     ]
    }
   ],
   "source": [
    "inv_z = (model.bln.inverse(z.numpy(force=True), alphas=[alpha]*depth, \n",
    "                                     inverse_activation_fns=[linen.relu]*depth, \n",
    "                                     Lambdas=[Lambda]*depth, iterations=[max_iter]*depth))\n",
    "print(f'inverse z: {inv_z} with shape {inv_z.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if inverse is correct - Ideally, the output should match z with minimal error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device of inv_z_torch: cuda:0\n",
      "device of model: cuda:0\n",
      "Mean diff between PLNet(inv(z) and z): 1.0113153621205129e-05 with shape torch.Size([10])\n",
      "tensor([7.0828e-06, 1.7102e-05, 1.4842e-05, 6.5778e-06, 9.1296e-06, 8.1695e-06,\n",
      "        1.0929e-05, 6.7303e-06, 9.0258e-06, 1.1543e-05], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "model.to(device)\n",
    "inv_z_torch = torch.tensor(np.array(inv_z), dtype=torch.float32).to(device)\n",
    "print(f\"device of inv_z_torch: {inv_z_torch.device}\")\n",
    "print(f\"device of model: {next(model.parameters()).device}\")\n",
    "with torch.no_grad():\n",
    "\toutput = model.bln(inv_z_torch)\n",
    "\n",
    "diff = torch.norm(z.to(device) - output, dim=1)\n",
    "print(f'Mean diff between PLNet(inv(z) and z): {diff.mean()} with shape {diff.shape}')\n",
    "\n",
    "print(diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plnet_mjx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
